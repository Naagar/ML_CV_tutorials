# -*- coding: utf-8 -*-
"""IR_Security_Risks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aWrqRqLNWt7m-Pe7g_ZO0Mf2KTuEetHc

## What is OWASP?
OWASP stands for the Open Web Application Security Project, an online community that produces articles, methodologies, documentation, tools, and technologies in the field of web application security.

## What is the OWASP Top 10?
OWASP Top 10 is the list of the 10 most common application vulnerabilities. It also shows their risks, impacts, and countermeasures. Updated every three to four years, the latest OWASP vulnerabilities list was released in 2017. Let’s dive into it!

##The Top 10 OWASP vulnerabilities in 2021 are:

*   Injection.
*   Broken authentication.
*   Sensitive data exposure.
*   XML external entities (XXE)
*   Broken access control.
*   Security misconfigurations.
*   Cross site scripting (XSS)
*   Insecure deserialization.
*   Using components with known vulnerabilities
*   Insufficient logging and monitoring

# Web Crawler --
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))
nltk.download("words")



import requests #for making HTTP requests in Python
from bs4 import BeautifulSoup # pulling data from HTML or XML files

query = "Injection"
# https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=Injection&search_type=all&isCpeNameSearch=false

# r = requests.get('https://www.google.com/search?q={}'.format(query))
r = requests.get('https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=Injection&search_type=all&isCpeNameSearch=false')

soup = BeautifulSoup(r.text, "html.parser") # Using BeautifulSoup we will parse the web page to extraxt the liks from the page
# soup
# print(soup.prettify())

# Extract the useful liks from list of links
links = []
for item1 in soup.find_all('li'):
  for item2 in soup.find_all('a'):
    links.append(item2.get('href'))
# links </li>
print(links)

# Finding the indexed links for Web crawling
final = []
# for item in links:
#   if item != None :
#     print(item)
#     print(len(item))
for item in links:
  if item != None :
    if len(item) >= 120:
      # stri = item[0:11]
      # if stri == "/vuln/search/":
      final.append(item)
final       # final list of liks

webpage1 = requests.get('https://nvd.nist.gov' + final[0]) #final[0] refers to the first web page link
webpagetext = BeautifulSoup(webpage1.text, "html.parser")
all_p = webpagetext.find_all('p')
text_NVD = ""
for item in all_p:
    text_NVD = text_NVD + item.get_text()
print(text_NVD)

"""## Text (sentence) from wiki for "Injection""""

query = "Injection" # sql Injection
webpage_wiki = requests.get('https://en.wikipedia.org/wiki/SQL_injection')
webpage_wiki_sde = requests.get('https://en.wikipedia.org/wiki/Data_breach') # sensitive data exposure (sde)
# webpage1 = requests.get('https://nvd.nist.gov/' + final[0])

webpagetext = BeautifulSoup(webpage_wiki.text, "html.parser")
all_p = webpagetext.find_all('p')
text_wiki = ""
for item in all_p:
    text_wiki = text_wiki + item.get_text()
print(text_wiki)

query = "sensitive data exposure" # sensitive data exposure

webpage_wiki_sde = requests.get('https://en.wikipedia.org/wiki/Data_breach') # sensitive data exposure (sde)
# webpage1 = requests.get('https://nvd.nist.gov/' + final[0])

webpagetext = BeautifulSoup(webpage_wiki_sde.text, "html.parser")
all_p = webpagetext.find_all('p')
text_wiki_sde = ""
for item in all_p:
    text_wiki_sde = text_wiki_sde + item.get_text()
print(text_wiki_sde)

import gensim
# Operating System
import os
# Regular Expression
import re
# nltk packages
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
# Basic Packages
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# PCA Package
from sklearn.decomposition import PCA
stemmer = SnowballStemmer("english")

stopWords = pd.read_csv('stopwords.txt').values

class MySentences(object):
    def __init__(self, fnamelist):
        self.fnamelist = fnamelist
        # Creating a set of vocabulary
        self.vocabulary = set([])

    def __iter__(self):
        for fname in self.fnamelist:
            for line in open(fname, encoding='latin1'):
                # Find all the words that has letters from 2 - 15. If the words are longer than that ignore.
                words = re.findall(r'(\b[A-Za-z][a-z]{2,15}\b)', line)
                # Stemming a word.
                words = [ stemmer.stem(word.lower()) for word in words if not word.lower() in stopWords]
                for word in words:
                    self.vocabulary.add(word)
                yield words
sentences_nvd = MySentences(text_NVD) # a memory-friendly iterator 
sentences_wiki = MySentences(text_wiki) # a memory-friendly iterator 
sentences_wiki_sde = MySentences(text_wiki_sde) # a memory-friendly iterator

from nltk.tokenize import word_tokenize, sent_tokenize
tokenized_nvd = sent_tokenize(text_NVD)
tokenized_wiki = sent_tokenize(text_wiki)
tokenized_wiki_sde = sent_tokenize(text_wiki_sde)



"""Finally, we create an inverted index. We maintain a secondary list called ispresent store the words that are already present in the inverted index. The variable invind stores the inverted index."""

lemmatizer = WordNetLemmatizer() #we instantiate the lemmatizer using the WordNetLemmatizer library
stop = set(stopwords.words('english'))
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]
  #creating a second list called lemm that stores the lemmatized words
  lemm = [lemmatizer.lemmatize(item) for item in tokenstop]
  for word in lemm:
    if word not in invind.keys():
      ispresent[word] = [y]
      invind[word] = [[y,1]]
    elif y not in ispresent[word]:
      invind[word].append([y,1])
      ispresent[word].append(y)
      else:
        for i in range(len(invind[word])):
        if invind[word][i][0]==y:
        invind[word][i][1]+=1

"""### Let’s create some sentences, initialize our model, and encode the sentences:

Take a sentence, convert it into a vector.
Take many other sentences, and convert them into vectors.
Find sentences that have the smallest distance (Euclidean) or smallest angle (cosine similarity) between them — more on that here.
We now have a measure of semantic similarity between sentences — easy!
"""

!pip install sentence-transformers
!pip install huggingface-hub==0.0.12

# Write a few sentences to encode (sentences 0 and 2 are both similar):
# sentences = [
#     "Three years later, the coffin was still full of Jello.",
#     "The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.",
#     "The person box was packed with jelly many dozens of months later.",
#     "He found a leprechaun in his walnut shell."
# ]
sentences = [text_NVD, text_wiki, text_wiki_sde]
# sentences = sentences.append(text_wiki)
# print(sentences[0])

# Initialize our model:
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('bert-base-nli-mean-tokens')

sentence_embeddings = model.encode(sentences)
sentence_embeddings.shape ##

"""Encode the sentences:

Great, we now have four sentence embeddings — each containing 768 values.

###  Now what we do is take those embeddings and find the cosine similarity between each. So for sentence 0: 
We can find the most similar sentence using:
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)

"""| Index | Sentence | Similarity |
| --- | --- | --- |
| 1 | Test from NVD for Injuctions. | 0.8555658 |
| 2 |  Text from Wiki page for Injections.     | 0.6837034 | 
| 3 |   Text from NVD for Sensitive data exposure (sde)   | 0.5548 |

## Simillar vulnerabilities for (Injection)
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarity(
    [sentence_embeddings[2]],
    sentence_embeddings[1:]
)

"""### Additionally, before the mean pooling operation, we need to create last_hidden_state, which we do like so:

"""

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')
model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')

sentences = [
    "Three years later, the coffin was still full of Jello.",
    "The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.",
    "The person box was packed with jelly many dozens of months later.",
    "He found a leprechaun in his walnut shell."
]

# initialize dictionary to store tokenized sentences
tokens = {'input_ids': [], 'attention_mask': []}

for sentence in sentences:
    # encode each sentence and append to dictionary
    new_tokens = tokenizer.encode_plus(sentence, max_length=128,
                                       truncation=True, padding='max_length',
                                       return_tensors='pt')
    tokens['input_ids'].append(new_tokens['input_ids'][0])
    tokens['attention_mask'].append(new_tokens['attention_mask'][0])

# reformat list of tensors into single tensor
tokens['input_ids'] = torch.stack(tokens['input_ids'])
tokens['attention_mask'] = torch.stack(tokens['attention_mask'])

# We process these tokens through our model:

outputs = model(**tokens)
outputs.keys()

# The dense vector representations of our text are contained within the outputs 
# 'last_hidden_state' tensor, which we access like so:
embeddings = outputs.last_hidden_state
embeddings

# 
embeddings.shape

"""After we have produced our dense vectors embeddings, we need to perform a mean pooling operation to create a single vector encoding (the sentence embedding).


*  To do this mean pooling operation, we will need to multiply each value in our embeddings tensor by its respective attention_mask value — so that we ignore non-real tokens.

"""

# To perform this operation, we first resize our attention_mask tensor:
attention_mask = tokens['attention_mask']
attention_mask.shape

mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
mask.shape

mask

# Each vector above represents a single token attention mask  
#  -each token now has a vector of size 768 representing it's attention_mask status.
# Then we multiply the two tensors to apply the attention mask:


masked_embeddings = embeddings * mask
masked_embeddings.shape

masked_embeddings
# Then we sum the remained of the embeddings along axis 1:
summed = torch.sum(masked_embeddings, 1)
summed.shape

#Then sum the number of values that must be given attention in each position of the tensor:
summed_mask = torch.clamp(mask.sum(1), min=1e-9)
summed_mask.shape

summed_mask

"""Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask:"""

mean_pooled = summed / summed_mask
mean_pooled

"""Once we have our dense vectors, we can calculate the cosine similarity between each — which is the same logic we used before:"""

from sklearn.metrics.pairwise import cosine_similarity

# Let's calculate cosine similarity for sentence 0:

# convert from PyTorch tensor to numpy array
mean_pooled = mean_pooled.detach().numpy()

# calculate
cosine_similarity(
    [mean_pooled[0]],
    mean_pooled[1:]
)

"""These similarities translate to:"""

from scipy import spatial
from sent2vec.vectorizer import Vectorizer

sentences = [
    "This is an awesome book to learn NLP.",
    "DistilBERT is an amazing NLP model.",
    "We can interchangeably use embedding, encoding, or vectorizing.",
]

vectorizer = Vectorizer()
vectorizer.bert(sentences)
vectors_bert = vectorizer.vectors

dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])
dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])
print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))
# dist_1: 0.043, dist_2: 0.192

# see data, choose the th, range, stats, Simillarty score
# web crawling,Rl page to page, 
How dose the model to get rewarded, papers, reviews

"""## For every document, we first tokenize the document into individual words."""



"""## CVSS score"""

!pip install cvss

from cvss import CVSS2, CVSS3


vector = 'AV:L/AC:L/Au:M/C:N/I:P/A:C/E:U/RL:W/RC:ND/CDP:L/TD:H/CR:ND/IR:ND/AR:M'
c = CVSS2(vector)
print(vector)
print(c.clean_vector())
print(c.scores())

print()

vector = 'CVSS:3.0/S:C/C:H/I:H/A:N/AV:P/AC:H/PR:H/UI:R/E:H/RL:O/RC:R/CR:H/IR:X/AR:X/MAC:H/MPR:X/MUI:X/MC:L/MA:X'
c = CVSS3(vector)
print(vector)
print(c.clean_vector())
print(c.scores())
print(c.severities())



import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd
stopwords = pd.read_csv('stopwords.txt').values
# stop = set(stopwords.('english')) #stopwords list
# doc is a list of many document texts 
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]


lemmatizer = WordNetLemmatizer() #we instantiate the lemmatizer using the WordNetLemmatizer library
stop = set(stopwords.words('english'))
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]
  #creating a second list called lemm that stores the lemmatized words
  lemm = [lemmatizer.lemmatize(item) for item in tokenstop]

"""# Deep-Deep: Adaptive Crawler (using RL)
Deep-Deep is a Scrapy-based crawler which uses Reinforcement Learning methods to learn which links to follow.

It is called Deep-Deep, but it doesn't use Deep Learning, and it is not only for Deep web. Weird

You can use deep-deep to just run adaptive crawls, updating link model and collecting crawled data at the same time. But in some cases it is more efficient to first train a link model with deep-deep, and then use this model in another crawler. Deep-deep uses a lot of memory to store page and link features, and more CPU to update the link model. So if the link model is general enough to freeze it, you can run a more efficient crawl. Or you might want to just use deep-deep link model in an existing project.
[link](https://github.com/TeamHG-Memex/deep-deep)

## url-summary
Show summary of a large number of URLs in a Jupyter Notebook: analyze domains, paths, query keys and values. This is useful if you want to have a quick glance at URLs obtained by crawling.

[link](https://github.com/TeamHG-Memex/url-summary)
"""

# NVD , input/ output, Crawling (decision) random web crawling, sq pro,https://www.virustotal.com/gui/

"""Build a crawler, visit page(input value) and decide- related and compare to wiki_text, (reward) """

!pip install gym

"""# Helper finctions"""

import gym
import numpy as np
import sys
from gym.envs.toy_text import discrete

UP = 0
RIGHT = 1
DOWN = 2
LEFT = 3

class WindyGridworldEnv(discrete.DiscreteEnv):

    metadata = {'render.modes': ['human', 'ansi']}

    def _limit_coordinates(self, coord):
        coord[0] = min(coord[0], self.shape[0] - 1)
        coord[0] = max(coord[0], 0)
        coord[1] = min(coord[1], self.shape[1] - 1)
        coord[1] = max(coord[1], 0)
        return coord

    def _calculate_transition_prob(self, current, delta, winds):
        new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)]
        new_position = self._limit_coordinates(new_position).astype(int)
        new_state = np.ravel_multi_index(tuple(new_position), self.shape)
        is_done = tuple(new_position) == (3, 7)
        return [(1.0, new_state, -1.0, is_done)]

    def __init__(self):
        self.shape = (7, 10)

        nS = np.prod(self.shape)
        nA = 4

        # Wind strength
        winds = np.zeros(self.shape)
        winds[:,[3,4,5,8]] = 1
        winds[:,[6,7]] = 2

        # Calculate transition probabilities
        P = {}
        for s in range(nS):
            position = np.unravel_index(s, self.shape)
            P[s] = { a : [] for a in range(nA) }
            P[s][UP] = self._calculate_transition_prob(position, [-1, 0], winds)
            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], winds)
            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0], winds)
            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1], winds)

        # We always start in state (3, 0)
        isd = np.zeros(nS)
        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0

        super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)

    def render(self, mode='human', close=False):
        self._render(mode, close)

    def _render(self, mode='human', close=False):
        if close:
            return

        outfile = StringIO() if mode == 'ansi' else sys.stdout

        for s in range(self.nS):
            position = np.unravel_index(s, self.shape)
            # print(self.s)
            if self.s == s:
                output = " x "
            elif position == (3,7):
                output = " T "
            else:
                output = " o "

            if position[1] == 0:
                output = output.lstrip()
            if position[1] == self.shape[1] - 1:
                output = output.rstrip()
                output += "\n"

            outfile.write(output)
        outfile.write("\n")

import matplotlib
import numpy as np
import pandas as pd
from collections import namedtuple
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

EpisodeStats = namedtuple("Stats",["episode_lengths", "episode_rewards"])

def plotting(env, estimator, num_tiles=20):
    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_tiles)
    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_tiles)
    X, Y = np.meshgrid(x, y)
    Z = np.apply_along_axis(lambda _: -np.max(estimator.predict(_)), 2, np.dstack([X, Y]))

    fig = plt.figure(figsize=(10, 5))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
                           cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)
    ax.set_xlabel('Position')
    ax.set_ylabel('Velocity')
    ax.set_zlabel('Value')
    ax.set_title("Mountain \"Cost To Go\" Function")
    fig.colorbar(surf)
    plt.show()


def plot_value_function(V, title="Value Function"):
    """
    Plots the value function as a surface plot.
    """
    min_x = min(k[0] for k in V.keys())
    max_x = max(k[0] for k in V.keys())
    min_y = min(k[1] for k in V.keys())
    max_y = max(k[1] for k in V.keys())

    x_range = np.arange(min_x, max_x + 1)
    y_range = np.arange(min_y, max_y + 1)
    X, Y = np.meshgrid(x_range, y_range)

    # Find value for all (x, y) coordinates
    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))
    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))

    def plot_surface(X, Y, Z, title):
        fig = plt.figure(figsize=(20, 10))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)
        ax.set_xlabel('Player Sum')
        ax.set_ylabel('Dealer Showing')
        ax.set_zlabel('Value')
        ax.set_title(title)
        ax.view_init(ax.elev, -120)
        fig.colorbar(surf)
        plt.show()

    plot_surface(X, Y, Z_noace, "{} (No Usable Ace)".format(title))
    plot_surface(X, Y, Z_ace, "{} (Usable Ace)".format(title))



def plot_episode_stats(stats, smoothing_window=10, noshow=False):
    # Plot the episode length over time
    fig1 = plt.figure(figsize=(10,5))
    plt.plot(stats.episode_lengths)
    plt.xlabel("Episode")
    plt.ylabel("Episode Length")
    plt.title("Episode Length over Time")
    if noshow:
        plt.close(fig1)
    else:
        plt.show(fig1)

    # Plot the episode reward over time
    fig2 = plt.figure(figsize=(10,5))
    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()
    plt.plot(rewards_smoothed)
    plt.xlabel("Episode")
    plt.ylabel("Episode Reward (Smoothed)")
    plt.title("Episode Reward over Time (Smoothed over window size {})".format(smoothing_window))
    if noshow:
        plt.close(fig2)
    else:
        plt.show(fig2)

    # Plot time steps and episode number
    fig3 = plt.figure(figsize=(10,5))
    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))
    plt.xlabel("Time Steps")
    plt.ylabel("Episode")
    plt.title("Episode per time step")
    if noshow:
        plt.close(fig3)
    else:
        plt.show(fig3)

    return fig1, fig2, fig3

"""# importing lib"""

import gym
import itertools
import matplotlib
import matplotlib.style
import numpy as np
import pandas as pd
import sys
  
  
from collections import defaultdict
# from windy_gridworld import WindyGridworldEnv
# import plotting
  
matplotlib.style.use('ggplot')

# Create gym environment.
env = WindyGridworldEnv()

def createEpsilonGreedyPolicy(Q, epsilon, num_actions):
    """
    Creates an epsilon-greedy policy based
    on a given Q-function and epsilon.
       
    Returns a function that takes the state
    as an input and returns the probabilities
    for each action in the form of a numpy array 
    of length of the action space(set of possible actions).
    """
    def policyFunction(state):
   
        Action_probabilities = np.ones(num_actions,
                dtype = float) * epsilon / num_actions
                  
        best_action = np.argmax(Q[state])
        Action_probabilities[best_action] += (1.0 - epsilon)
        return Action_probabilities
   
    return policyFunction

def qLearning(env, num_episodes, discount_factor = 1.0,
                            alpha = 0.6, epsilon = 0.1):
    """
    Q-Learning algorithm: Off-policy TD control.
    Finds the optimal greedy policy while improving
    following an epsilon-greedy policy"""
       
    # Action value function
    # A nested dictionary that maps
    # state -> (action -> action-value).
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
   
    # Keeps track of useful statistics
    stats = plotting.EpisodeStats(
        episode_lengths = np.zeros(num_episodes),
        episode_rewards = np.zeros(num_episodes))    
       
    # Create an epsilon greedy policy function
    # appropriately for environment action space
    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)
       
    # For every episode
    for ith_episode in range(num_episodes):
           
        # Reset the environment and pick the first action
        state = env.reset()
           
        for t in itertools.count():
               
            # get probabilities of all actions from current state
            action_probabilities = policy(state)
   
            # choose action according to 
            # the probability distribution
            action = np.random.choice(np.arange(
                      len(action_probabilities)),
                       p = action_probabilities)
   
            # take action and get reward, transit to next state
            next_state, reward, done, _ = env.step(action)
   
            # Update statistics
            stats.episode_rewards[ith_episode] += reward
            stats.episode_lengths[ith_episode] = t
               
            # TD Update
            best_next_action = np.argmax(Q[next_state])    
            td_target = reward + discount_factor * Q[next_state][best_next_action]
            td_delta = td_target - Q[state][action]
            Q[state][action] += alpha * td_delta
   
            # done is True if episode terminated   
            if done:
                break
                   
            state = next_state
       
    return Q, stats

Q, stats = qLearning(env, 1000)

plotting.plot_episode_stats(stats)

