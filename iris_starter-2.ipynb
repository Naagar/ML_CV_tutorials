{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "\n",
    "#import required packages\n",
    "\n",
    "from numpy import mean, std\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from Google Drive\n",
    "url = 'https://drive.google.com/uc?id=' + '1BmICPGpdRg1dPmXi0G3Fe5IWs1MobCu8' #(URI ID)\n",
    "output = '/home/kiran/ta/data/iris.data' # Destination directory\n",
    "gdown.download(url, output, quiet=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data using pandas read_csv method\n",
    "df=read_csv(output, sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "#Creating a dataframe\n",
    "df = pd.DataFrame(iris.data)\n",
    "df['class'] = iris.target\n",
    "y = df['class'].values\n",
    "x = df.drop(['class'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 Display the first few rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2: Visualize the histogram of data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2: Plot the histogram of values from any other attribute of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our model’s performance later, we will need to divide the data set into two parts: a training set and a\n",
    "test set. The first is used to train the system, while the second is used to evaluate the learned or trained system.\n",
    "\n",
    "Sklearn provides us with an easy way to randomly break up our data. We have decided to split the data with 20% as test and 80% as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters for Logistic Regression\n",
    "1. penalty: Used to specify the norm used in the penalization. \n",
    "2. C: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "3. max_iter: Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "Epoch: One Epoch is when an entire dataset is passed forward and backward through the classifier / neural network only once.\n",
    "\n",
    "Iterations: Iterations is the number of batches that is passed forward and backward through the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delcare a Logistic Regression classifier\n",
    "clf = LogisticRegression(penalty='l2',C=1.0, max_iter=10000)\n",
    "# Train the classifier until max_iterations\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters for Decision Trees\n",
    "1. criterion: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    " - Gini: The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.\n",
    " - Entropy is a measure of information that indicates the disorder of the features with the target.\n",
    "2. splitter: The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "3. max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "4. min_samples_split: The minimum number of samples required to split an internal node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2)\n",
    "# Create Decision Tree on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters for SVM\n",
    "1. kernel: (‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed') Specifies the kernel type to be used in the algorithm.\n",
    "2. degree: Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "3. gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "4. max_iter: Hard limit on iterations within solver, or -1 for no limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the SVM classifier\n",
    "clf = SVC(kernel='poly', degree=3, max_iter=300000)\n",
    "# Train until max iterations\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters for LinearRegression:\n",
    "1. fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)\n",
    "2. normalize: If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes:\n",
    "The likelihood of the features is assumed to be Gaussian. Paramters are\n",
    "1. priors: Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "2. var_smoothing: Portion of the largest variance of all features that is added to variances for calculation stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 Implement KNN using sklearn\n",
    "# Show results for using both Euclidean distance and Manhattan Distance metric for the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 Implement two other classifiers of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict using our trained model on the test set we created and evaluate our model on unforeseen data.\n",
    "The performance will be reflected in various standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting for test data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating results for various evaluation metric\n",
    "precision = precision_score(y_test,y_pred, average='micro')\n",
    "recall = recall_score(y_test,y_pred, average='micro')\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test,y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5: Insert the code for visualizing the comfusion matrix here\n",
    "#store the confusion matrix in the variable cm with dim:2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, range(cm.shape[0]), range(cm.shape[1]))\n",
    "sns.set(font_scale=1) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement K-fold validation and compare the perfromance with 80-20 random split (using sklearn methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
